---
title: "强化学习系列 - 1. DQN"
summary: Learn About DQN in RL
date: 2025-02-16
weight: 1
aliases: ["/RL-DQN"]
tags: ["DQN"]
categories: ["RL"]
author: ["Yongliang Yang"]
cover:
  image: images/papermod-cover.png
---




## 1.DQN 算法流程

```python             
# 0. 初始化
# 0.1 初始化环境 env
env =
# 0.2 初始化 DQN
q(s, a; θ) = 
q_(s, a; θ_) = 
# 0.3 初始化经验回放库 Buffer
replay_buffer = 

# 主循环：N 个 episode
for episode = 1 to M:

    # 1. 初始化
    s_t = env.reset()  # 初始化状态 s_t
    episode_return = 0 # 初始化累计回报 episode_return

    # 每个 episode 
    done = False
    while not done:

        # 2. 以 ε-贪婪策略生成动作 a_t
        a_t = argmax_a Q(s_t, a; θ) or random
        
        # 3. 执行动作 a_t，得到奖励 r_t、新状态 s_{t+1}、以及游戏结束标志 done
        s_{t+1}, r_t, done, _ = env.step(a_t)
        
        # 4. 存储一次交互数据 (s_t, a_t, r_t, φ_{t+1}, done) 至经验回放库 replay_buffer
        replay_buffer.append( (s_t, a_t, r_t, s_{t+1}, done) )
        
        # 5. 当 replay_buffer 数据量满足最小长度 (比如500)，从 replay_buffer 中随机抽取一批数据 batch <-  {s_j, a_j, r_j, s_{j+1}, done_j}, j = 1, ..., minimal_size（ batch 数据量为 minimal_size ）
        if len(Buffer) > 500:
            batch <-  replay_buffer( 1, ..., minimal_size ) 
        
        # 6. 利用采样数据 batch 训练 DQN
        DQN.train(batch)        
        
        # 7. 更新状态 s_t <- s_{t+1}
        s_t = s_{t+1}

        # 8. 更新累计回报
        episode_return += reward # 更新累计回报
        
        
        if done:
            break

```
